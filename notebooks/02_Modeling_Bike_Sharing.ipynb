{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "164184e5-7d17-404c-b18b-87ef2fa40e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"BikeSharing_Modeling\").getOrCreate()\n",
    "joined_df = spark.read.parquet(\"data/joined_bike_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612541c2-7c81-4e0a-a956-6f90335e099c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Features\n",
    "categorical_cols = [\"season\", \"yr\", \"mnth\", \"hr\", \"holiday\", \"weekday\", \"workingday\", \"weathersit\"]\n",
    "numerical_cols = [\"temp\", \"atemp\", \"hum\", \"windspeed\"]\n",
    "target_col = \"cnt\"\n",
    "\n",
    "# Index + OneHot\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_idx\") for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{col}_idx\", outputCol=f\"{col}_ohe\") for col in categorical_cols]\n",
    "\n",
    "# Assemble all features\n",
    "feature_cols = [f\"{col}_ohe\" for col in categorical_cols] + numerical_cols\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "  \n",
    "# Scale\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Full preprocessing pipeline\n",
    "preprocessing = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
    "model_input = preprocessing.fit(joined_df).transform(joined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eedded34-6f52-49d9-893a-df3d7c6bc64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = model_input.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee31d1f9-fd5f-491c-977c-63289503cf9e",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a8d59c-b11c-4d77-b941-a712faea53fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 102.66531858745806\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"cnt\")\n",
    "lr_model = lr.fit(train_df)\n",
    "lr_preds = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluation\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"cnt\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "print(\"RMSE:\", evaluator.evaluate(lr_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b23f50-e3f8-4b2b-b38b-e03b3c3d29b9",
   "metadata": {},
   "source": [
    "Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d58d8a66-0e39-4272-9490-7d1dd5ac3ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree RMSE: 129.00656526720593\n",
      "Decision Tree R²: 0.5124402234396651\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol=\"scaled_features\", labelCol=\"cnt\")\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_preds = dt_model.transform(test_df)\n",
    "\n",
    "dt_rmse = evaluator.evaluate(dt_preds)\n",
    "dt_r2 = evaluator.setMetricName(\"r2\").evaluate(dt_preds)\n",
    "\n",
    "print(\"Decision Tree RMSE:\", dt_rmse)\n",
    "print(\"Decision Tree R²:\", dt_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f437e8-43cd-4755-ae45-5b323b5307b1",
   "metadata": {},
   "source": [
    "Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ed68336-18a8-41d5-8e1f-3f1af6219e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest RMSE: 0.5495092615117809\n",
      "Random Forest R²: 0.5495092615117809\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"scaled_features\", labelCol=\"cnt\", numTrees=100)\n",
    "rf_model = rf.fit(train_df)\n",
    "rf_preds = rf_model.transform(test_df)\n",
    "\n",
    "rf_rmse = evaluator.evaluate(rf_preds)\n",
    "rf_r2 = evaluator.setMetricName(\"r2\").evaluate(rf_preds)\n",
    "\n",
    "print(\"Random Forest RMSE:\", rf_rmse)\n",
    "print(\"Random Forest R²:\", rf_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224279b-5075-4314-a83f-f1d2841e88fc",
   "metadata": {},
   "source": [
    "Gradient-Boosted Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2a626b9-a20c-4535-b7ea-340e6bce7592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT RMSE: 0.8808462474791239\n",
      "GBT R²: 0.8808462474791239\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"scaled_features\", labelCol=\"cnt\", maxIter=50)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_preds = gbt_model.transform(test_df)\n",
    "\n",
    "gbt_rmse = evaluator.evaluate(gbt_preds)\n",
    "gbt_r2 = evaluator.setMetricName(\"r2\").evaluate(gbt_preds)\n",
    "\n",
    "print(\"GBT RMSE:\", gbt_rmse)\n",
    "print(\"GBT R²:\", gbt_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6372a7-26fd-49d2-bebb-ac0a9a505576",
   "metadata": {},
   "source": [
    "Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4fe3f23-cfa2-438c-b8e4-e3ef1bcfdc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Comparison ---\n",
      "Linear Regression    | RMSE: 0.69 | R²: 0.6912\n",
      "Decision Tree        | RMSE: 0.73 | R²: 0.7270\n",
      "Random Forest        | RMSE: 0.76 | R²: 0.7645\n",
      "Gradient BoostedTree | RMSE: 0.79 | R²: 0.7923\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Model Comparison ---\")\n",
    "print(f\"Linear Regression    | RMSE: {evaluator.evaluate(lr_preds):.2f} | R²: {evaluator.setMetricName('r2').evaluate(lr_preds):.4f}\")\n",
    "print(f\"Decision Tree        | RMSE: {dt_rmse:.2f} | R²: {dt_r2:.4f}\")\n",
    "print(f\"Random Forest        | RMSE: {rf_rmse:.2f} | R²: {rf_r2:.4f}\")\n",
    "print(f\"Gradient BoostedTree | RMSE: {gbt_rmse:.2f} | R²: {gbt_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b15877-9590-4e11-ae08-181d200133fe",
   "metadata": {},
   "source": [
    " Hyperparameter Tuning – Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b4e5e2-f7ab-4b43-9f52-6fa21b03a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"cnt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62733454-6b89-4f2a-a032-80a7d6aa1d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned LR → RMSE: 102.669, R²: 0.6912\n",
      "Best regParam:      0.01\n",
      "Best elasticNetParam: 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning   import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# 1. Split the *preprocessed* data (with scaled_features)  \n",
    "train_data, test_data = model_input.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 2. Define the model using the correct features column  \n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"cnt\")\n",
    "\n",
    "# 3. Build a small grid of hyperparameters  \n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam,      [0.01, 0.1, 0.5])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])  # Ridge → ElasticNet → Lasso\n",
    "             .build())\n",
    "\n",
    "# 4. Define the evaluator  \n",
    "evaluator = RegressionEvaluator(labelCol=\"cnt\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# 5. Set up TrainValidationSplit  \n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=evaluator,\n",
    "                           trainRatio=0.8,\n",
    "                           seed=42)\n",
    "\n",
    "# 6. Fit on train_data  \n",
    "lr_tuned = tvs.fit(train_data)\n",
    "\n",
    "# 7. Get best model & evaluate on test_data  \n",
    "best_lr = lr_tuned.bestModel\n",
    "preds = best_lr.transform(test_data)\n",
    "\n",
    "rmse = evaluator.evaluate(preds)\n",
    "r2   = evaluator.setMetricName(\"r2\").evaluate(preds)\n",
    "\n",
    "print(f\"Tuned LR → RMSE: {rmse:.3f}, R²: {r2:.4f}\")\n",
    "print(\"Best regParam:     \", best_lr.getOrDefault(\"regParam\"))\n",
    "print(\"Best elasticNetParam:\", best_lr.getOrDefault(\"elasticNetParam\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6ddd9-7e91-4d74-8e68-9f07178a20ad",
   "metadata": {},
   "source": [
    "Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "312fffe9-7658-40dc-8b5a-6e79ab512075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol=\"scaled_features\", labelCol=\"cnt\")\n",
    "\n",
    "paramGrid_dt = (ParamGridBuilder()\n",
    "                .addGrid(dt.maxDepth, [3, 5, 10])\n",
    "                .addGrid(dt.minInstancesPerNode, [1, 2, 4])\n",
    "                .build())\n",
    "\n",
    "tvs_dt = TrainValidationSplit(estimator=dt,\n",
    "                              estimatorParamMaps=paramGrid_dt,\n",
    "                              evaluator=evaluator,\n",
    "                              trainRatio=0.8,\n",
    "                              seed=42)\n",
    "\n",
    "dt_model = tvs_dt.fit(train_data)\n",
    "dt_preds = dt_model.bestModel.transform(test_data)\n",
    "dt_rmse  = evaluator.evaluate(dt_preds)\n",
    "dt_r2    = evaluator.setMetricName(\"r2\").evaluate(dt_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99984fc9-a27c-42db-bbbd-4ccd2f52fbb2",
   "metadata": {},
   "source": [
    "Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ead81e86-739f-44b3-98a2-cc80b9ec65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol=\"scaled_features\", labelCol=\"cnt\")\n",
    "\n",
    "paramGrid_rf = (ParamGridBuilder()\n",
    "                .addGrid(rf.numTrees, [10, 50])\n",
    "                .addGrid(rf.maxDepth, [5, 10])\n",
    "                .build())\n",
    "\n",
    "tvs_rf = TrainValidationSplit(estimator=rf,\n",
    "                              estimatorParamMaps=paramGrid_rf,\n",
    "                              evaluator=evaluator,\n",
    "                              trainRatio=0.8,\n",
    "                              seed=42)\n",
    "\n",
    "rf_model = tvs_rf.fit(train_data)\n",
    "rf_preds = rf_model.bestModel.transform(test_data)\n",
    "rf_rmse  = evaluator.evaluate(rf_preds)\n",
    "rf_r2    = evaluator.setMetricName(\"r2\").evaluate(rf_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff01ae10-6050-4be0-8ece-b783ef7fd8ae",
   "metadata": {},
   "source": [
    "Gradient Boosted Trees (GBT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6a81654-891d-4582-b749-c4a719e3e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"scaled_features\", labelCol=\"cnt\")\n",
    "\n",
    "paramGrid_gbt = (ParamGridBuilder()\n",
    "                 .addGrid(gbt.maxIter, [10, 20])\n",
    "                 .addGrid(gbt.maxDepth, [3, 5])\n",
    "                 .build())\n",
    "\n",
    "tvs_gbt = TrainValidationSplit(estimator=gbt,\n",
    "                               estimatorParamMaps=paramGrid_gbt,\n",
    "                               evaluator=evaluator,\n",
    "                               trainRatio=0.8,\n",
    "                               seed=42)\n",
    "\n",
    "gbt_model = tvs_gbt.fit(train_data)\n",
    "gbt_preds = gbt_model.bestModel.transform(test_data)\n",
    "gbt_rmse  = evaluator.evaluate(gbt_preds)\n",
    "gbt_r2    = evaluator.setMetricName(\"r2\").evaluate(gbt_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b80554-f10e-4a11-9762-655b54e3a900",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "735e4be4-710c-4d02-9579-7191a23ebbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure evaluator is reset to RMSE\n",
    "evaluator.setMetricName(\"rmse\")\n",
    "lr_rmse = evaluator.evaluate(lr_preds)\n",
    "\n",
    "# Then reset evaluator to R²\n",
    "evaluator.setMetricName(\"r2\")\n",
    "lr_r2 = evaluator.evaluate(lr_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2088a6f5-31ae-49f8-89f1-ed96bd146a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>102.665319</td>\n",
       "      <td>0.691218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.727018</td>\n",
       "      <td>0.727018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.764511</td>\n",
       "      <td>0.764511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBT</td>\n",
       "      <td>0.792280</td>\n",
       "      <td>0.792280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model        RMSE        R²\n",
       "0  Linear Regression  102.665319  0.691218\n",
       "1      Decision Tree    0.727018  0.727018\n",
       "2      Random Forest    0.764511  0.764511\n",
       "3                GBT    0.792280  0.792280"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {\n",
    "    \"Model\": [\"Linear Regression\", \"Decision Tree\", \"Random Forest\", \"GBT\"],\n",
    "    \"RMSE\":  [lr_rmse, dt_rmse, rf_rmse, gbt_rmse],\n",
    "    \"R²\":    [lr_r2, dt_r2, rf_r2, gbt_r2]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e6e69-1cad-4c51-93ed-5d5a996a19fe",
   "metadata": {},
   "source": [
    "hyper paramater tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce2d74f3-ffd0-4bc4-a440-effac6fba7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Tuned RMSE: 102.71859095063755\n",
      "Linear Regression Tuned R²: 0.6908976186597764\n",
      "Decision Tree Tuned RMSE: 86.10685246373131\n",
      "Decision Tree Tuned R²: 0.7827900712801916\n",
      "Random Forest Tuned RMSE: 89.65673028227825\n",
      "Random Forest Tuned R²: 0.7645113190300485\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Reuse the same evaluator for all\n",
    "evaluator = RegressionEvaluator(labelCol=\"cnt\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "def cross_validate(model, param_grid, train_data, evaluator, num_folds=3):\n",
    "    cv = CrossValidator(\n",
    "        estimator=model,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=num_folds,\n",
    "        parallelism=2\n",
    "    )\n",
    "    return cv.fit(train_data)\n",
    "\n",
    "# --- 1. Linear Regression ---\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='scaled_features', labelCol='cnt')\n",
    "\n",
    "lr_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.3]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "lr_cv_model = cross_validate(lr, lr_param_grid, train_data, evaluator)\n",
    "lr_best_model = lr_cv_model.bestModel\n",
    "lr_rmse = evaluator.evaluate(lr_best_model.transform(test_data))\n",
    "lr_r2 = RegressionEvaluator(labelCol=\"cnt\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(lr_best_model.transform(test_data))\n",
    "\n",
    "print(\"Linear Regression Tuned RMSE:\", lr_rmse)\n",
    "print(\"Linear Regression Tuned R²:\", lr_r2)\n",
    "\n",
    "# --- 2. Decision Tree ---\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol='scaled_features', labelCol='cnt')\n",
    "\n",
    "dt_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(dt.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "dt_cv_model = cross_validate(dt, dt_param_grid, train_data, evaluator)\n",
    "dt_best_model = dt_cv_model.bestModel\n",
    "dt_rmse = evaluator.evaluate(dt_best_model.transform(test_data))\n",
    "dt_r2 = RegressionEvaluator(labelCol=\"cnt\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(dt_best_model.transform(test_data))\n",
    "\n",
    "print(\"Decision Tree Tuned RMSE:\", dt_rmse)\n",
    "print(\"Decision Tree Tuned R²:\", dt_r2)\n",
    "\n",
    "# --- 3. Random Forest ---\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(featuresCol='scaled_features', labelCol='cnt')\n",
    "\n",
    "rf_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [20, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "rf_cv_model = cross_validate(rf, rf_param_grid, train_data, evaluator)\n",
    "rf_best_model = rf_cv_model.bestModel\n",
    "rf_rmse = evaluator.evaluate(rf_best_model.transform(test_data))\n",
    "rf_r2 = RegressionEvaluator(labelCol=\"cnt\", predictionCol=\"prediction\", metricName=\"r2\").evaluate(rf_best_model.transform(test_data))\n",
    "\n",
    "print(\"Random Forest Tuned RMSE:\", rf_rmse)\n",
    "print(\"Random Forest Tuned R²:\", rf_r2)\n",
    "\n",
    "# --- 4. GBT Regressor ---\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(featuresCol='scaled_features', labelCol='cnt')\n",
    "\n",
    "gbt_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [3, 5]) \\\n",
    "    .addGrid(gbt.maxIter, [20, 50]) \\\n",
    "    .addGrid(gbt.stepSize, [0.1, 0.2]) \\\n",
    "    .build()\n",
    "\n",
    "gbt_cv_model = cross_validate(gbt, gbt_param_grid, train_data, evaluator)\n",
    "gbt_best_model = gbt_cv_model.bestModel\n",
    "gbt_rmse = evaluator.evaluate(gbt_best_model.transform(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfecac-e62d-4fc2-8946-189b64b78f60",
   "metadata": {},
   "source": [
    "comparaison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "431f13c9-da20-4f7e-850b-6093b9c4fcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GBT</td>\n",
       "      <td>0.792280</td>\n",
       "      <td>0.792280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GBT (Tuned)</td>\n",
       "      <td>57.122435</td>\n",
       "      <td>0.792280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree (Tuned)</td>\n",
       "      <td>86.106852</td>\n",
       "      <td>0.782790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.764511</td>\n",
       "      <td>0.764511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (Tuned)</td>\n",
       "      <td>89.656730</td>\n",
       "      <td>0.764511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.727018</td>\n",
       "      <td>0.727018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>102.665319</td>\n",
       "      <td>0.691218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear Regression (Tuned)</td>\n",
       "      <td>102.718591</td>\n",
       "      <td>0.690898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model        RMSE        R²\n",
       "3                        GBT    0.792280  0.792280\n",
       "7                GBT (Tuned)   57.122435  0.792280\n",
       "5      Decision Tree (Tuned)   86.106852  0.782790\n",
       "2              Random Forest    0.764511  0.764511\n",
       "6      Random Forest (Tuned)   89.656730  0.764511\n",
       "1              Decision Tree    0.727018  0.727018\n",
       "0          Linear Regression  102.665319  0.691218\n",
       "4  Linear Regression (Tuned)  102.718591  0.690898"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_results = pd.DataFrame({\n",
    "    'Model': ['Linear Regression (Tuned)', 'Decision Tree (Tuned)', 'Random Forest (Tuned)', 'GBT (Tuned)'],\n",
    "    'RMSE': [lr_rmse, dt_rmse, rf_rmse, gbt_rmse],\n",
    "    'R²': [lr_r2, dt_r2, rf_r2, gbt_r2]\n",
    "})\n",
    "\n",
    "results_df = pd.concat([results_df, tuned_results], ignore_index=True)\n",
    "results_df.sort_values(by=\"R²\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae2cb7-cf59-475f-908a-2b401e024528",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2236ea-f0cb-4a4a-adad-3e0d83fe6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = joined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6ee900-9b76-4175-97ab-3e4042d697e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "total_rentals = df.select(sum(\"cnt\")).collect()[0][0]\n",
    "print(\"Total Rentals:\", total_rentals)\n",
    "\n",
    "# Save to CSV manually if needed:\n",
    "import pandas as pd\n",
    "pd.DataFrame([{\"Total Rentals\": total_rentals}]).to_csv(\"kpi_total_rentals.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da517b31-ab7c-476e-a6f7-b2204b8b669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "hourly_avg = df.groupBy(\"hr\").agg(avg(\"cnt\").alias(\"avg_rentals\")).orderBy(\"hr\")\n",
    "hourly_avg_pd = hourly_avg.toPandas()\n",
    "\n",
    "# Save\n",
    "hourly_avg_pd.to_csv(\"kpi_avg_rentals_by_hour.csv\", index=False)\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=hourly_avg_pd, x=\"hr\", y=\"avg_rentals\", palette=\"viridis\")\n",
    "plt.title(\"Average Rentals by Hour\")\n",
    "plt.xlabel(\"Hour of the Day\")\n",
    "plt.ylabel(\"Average Rentals\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3443f4-d91c-4ca0-b729-4c7b19738354",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal_totals = df.groupBy(\"season\").agg(sum(\"cnt\").alias(\"total_rentals\")).orderBy(\"season\")\n",
    "seasonal_pd = seasonal_totals.toPandas()\n",
    "\n",
    "# Save\n",
    "seasonal_pd.to_csv(\"kpi_rentals_by_season.csv\", index=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=seasonal_pd, x=\"season\", y=\"total_rentals\", palette=\"muted\")\n",
    "plt.title(\"Total Rentals by Season\")\n",
    "plt.xlabel(\"Season\")\n",
    "plt.ylabel(\"Total Rentals\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94361564-a94a-423c-9b56-ab86d79aaaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_binned = df.withColumn(\"temp_range\", when(df.temp < 0.3, \"Low\")\n",
    "                                        .when((df.temp >= 0.3) & (df.temp < 0.6), \"Medium\")\n",
    "                                        .otherwise(\"High\"))\n",
    "\n",
    "temp_range_avg = df_binned.groupBy(\"temp_range\").agg(avg(\"cnt\").alias(\"avg_rentals\"))\n",
    "temp_range_pd = temp_range_avg.toPandas()\n",
    "\n",
    "# Save\n",
    "temp_range_pd.to_csv(\"kpi_rentals_by_temp_range.csv\", index=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(data=temp_range_pd, x=\"temp_range\", y=\"avg_rentals\", order=[\"Low\", \"Medium\", \"High\"])\n",
    "plt.title(\"Average Rentals by Temperature Range\")\n",
    "plt.xlabel(\"Temperature Range\")\n",
    "plt.ylabel(\"Average Rentals\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1251886-81f3-4797-ab2c-0e1ebc8cc7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "# Total casual and registered users\n",
    "totals = joined_df.agg(\n",
    "    _sum(\"casual\").alias(\"casual_total\"),\n",
    "    _sum(\"registered\").alias(\"registered_total\")\n",
    ").collect()[0]\n",
    "\n",
    "casual_total = totals[\"casual_total\"]\n",
    "registered_total = totals[\"registered_total\"]\n",
    "\n",
    "# Then visualize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie([casual_total, registered_total], labels=[\"Casual\", \"Registered\"], autopct=\"%1.1f%%\", startangle=90)\n",
    "plt.title(\"Distribution of Casual vs Registered Users\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0a877-12a1-46d9-a0a3-77bb09372201",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"weathersit\").agg(avg(\"cnt\").alias(\"avg_rentals\")).orderBy(\"weathersit\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b91f54-00c6-43ee-ba64-d64f6b57b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"dteday\").agg(sum(\"cnt\").alias(\"daily_total\")).orderBy(\"daily_total\", ascending=False).show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89665e14-8d8c-439a-9089-86d89f0ad6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(avg(\"casual\").alias(\"avg_casual\"), avg(\"registered\").alias(\"avg_registered\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53dcb0-15d2-4810-84c5-262bbc9bbf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"weekday\").agg(avg(\"cnt\").alias(\"avg_rentals\")).orderBy(\"weekday\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b209860-ce17-4a56-9372-ee66eeba8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\"temp_range\", when(df.temp < 0.3, \"Low\")\n",
    "                                  .when((df.temp >= 0.3) & (df.temp < 0.6), \"Medium\")\n",
    "                                  .otherwise(\"High\"))\n",
    "\n",
    "df.groupBy(\"temp_range\").agg(avg(\"cnt\").alias(\"avg_rentals\")).orderBy(\"temp_range\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7bd9c-e80b-43fc-ac8a-a4ad1d10b0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc58136-2432-4332-8e5d-f158eeecb6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
